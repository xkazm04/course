{
  "monitors": [
    {
      "name": "[LLM] Gemini Response Latency Critical",
      "type": "metric alert",
      "query": "avg(last_5m):avg:llm.gemini.request.latency{service:oracle OR service:content-generator} > 10000",
      "message": "## High LLM Response Latency Detected\n\n**Current Latency:** {{value}} ms\n**Service:** {{service.name}}\n**Threshold:** 10,000ms\n\n### Impact\n- User experience degradation\n- Potential timeouts for learning path generation\n- Content generation jobs may fail\n\n### Runbook\n1. Check Gemini API status: https://status.cloud.google.com\n2. Review recent deployment changes\n3. Check for prompt complexity increases\n4. Consider implementing request caching\n5. Escalate if persists > 15 minutes\n\n@slack-llm-alerts",
      "tags": ["service:llm-observability", "severity:high", "team:ai-platform", "hackathon:datadog"],
      "options": {
        "thresholds": {
          "critical": 10000,
          "warning": 5000
        },
        "notify_no_data": true,
        "no_data_timeframe": 10,
        "include_tags": true
      }
    },
    {
      "name": "[LLM] Gemini API Error Rate Elevated",
      "type": "metric alert",
      "query": "sum(last_5m):sum:llm.gemini.errors{*}.as_count() / sum:llm.gemini.request.count{*}.as_count() * 100 > 5",
      "message": "## LLM API Error Rate Above Threshold\n\n**Error Rate:** {{value}}%\n**Threshold:** 5%\n\n### Common Error Types\n- `rate_limit`: API quota exceeded\n- `invalid_response`: Malformed LLM output\n- `parse_error`: JSON parsing failure\n- `timeout`: Request timeout\n- `safety_block`: Content filtered by safety\n\n### Runbook\n1. Check error breakdown in APM\n2. If rate_limit: Review quota in GCP Console\n3. If parse_error: Review prompt engineering\n4. If timeout: Check Gemini API latency\n5. Contact Vertex AI support if needed\n\n@slack-llm-alerts",
      "tags": ["service:llm-observability", "severity:critical", "team:ai-platform", "hackathon:datadog"],
      "options": {
        "thresholds": {
          "critical": 5,
          "warning": 2
        },
        "notify_no_data": false,
        "include_tags": true
      }
    },
    {
      "name": "[Cost] LLM Token Spend Anomaly",
      "type": "metric alert",
      "query": "avg(last_1h):anomalies(sum:llm.gemini.cost.usd{*}, 'agile', 3) >= 1",
      "message": "## Unusual LLM Cost Pattern Detected\n\n**Current Hourly Cost:** Anomaly detected\n\n### Potential Causes\n- Increased user activity\n- Prompt regression (excessive tokens)\n- Runaway content generation jobs\n- Possible abuse or attack\n\n### Runbook\n1. Review cost breakdown by operation\n2. Check for unusual session patterns\n3. Audit recent prompt changes\n4. Enable rate limiting if abuse suspected\n5. Set budget alerts in GCP\n\n@slack-finops",
      "tags": ["service:llm-observability", "team:finops", "hackathon:datadog"],
      "options": {
        "threshold_windows": {
          "trigger_window": "last_1h",
          "recovery_window": "last_1h"
        },
        "notify_no_data": false,
        "include_tags": true
      }
    },
    {
      "name": "[Jobs] Content Generation Failure Rate",
      "type": "metric alert",
      "query": "sum(last_15m):sum:content.job.duration{status:failed}.as_count() / sum:content.job.duration{*}.as_count() * 100 > 10",
      "message": "## Content Generation Jobs Failing\n\n**Failure Rate:** {{value}}%\n**Threshold:** 10%\n\n### Impact\n- Users cannot generate course content\n- Map nodes remain without courses\n- Platform value proposition degraded\n\n### Runbook\n1. Check job error messages in logs\n2. Review Gemini API response quality\n3. Verify database connectivity\n4. Check for prompt format issues\n5. Retry failed jobs if transient\n\n@slack-content-team",
      "tags": ["service:content-generator", "severity:high", "hackathon:datadog"],
      "options": {
        "thresholds": {
          "critical": 10,
          "warning": 5
        },
        "notify_no_data": false,
        "include_tags": true
      }
    },
    {
      "name": "[LLM] JSON Parse Errors Elevated",
      "type": "metric alert",
      "query": "sum(last_10m):sum:llm.response.parse_error{*}.as_count() > 10",
      "message": "## LLM Response Parse Errors\n\n**Parse Errors:** {{value}}\n**Time Window:** Last 10 minutes\n\nThis indicates the LLM is returning malformed JSON responses.\n\n### Runbook\n1. Check recent prompt changes\n2. Review model temperature settings\n3. Audit response format instructions in prompts\n4. Consider adding retry logic with stricter prompts\n5. Check if Gemini model version changed\n\n@slack-llm-alerts",
      "tags": ["service:llm-observability", "severity:medium", "hackathon:datadog"],
      "options": {
        "thresholds": {
          "critical": 10,
          "warning": 5
        },
        "notify_no_data": false,
        "include_tags": true
      }
    }
  ]
}
